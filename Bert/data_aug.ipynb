{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Citation:\n",
    "\n",
    "[1] NLTK. [Online]. Available: https://www.nltk.org/. [Accessed: 28-Jul-2022]. \n",
    "[2] “Pandas,” pandas. [Online]. Available: https://pandas.pydata.org/.\n",
    "[3] “SKLearn,” scikit-learn. [Online]. Available: https://scikit-learn.org/stable/.\n",
    "[4] “WordNet,” Princeton University. [Online]. Available: https://wordnet.princeton.edu/.\n",
    "[5] V. PRASANNA KUMAR and T. Patro, “Bert model with 0.845 accuracy,” Kaggle, 23-Aug-2020. [Online]. Available: https://www.kaggle.com/code/vpkprasanna/bert-model-with-0-845-accuracy/notebook.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Preprocessed Data for Augmentation.\n",
    "data_df =  pd.read_csv('../NER/train.csv',index_col=0)\n",
    "data_df['Remedies'] = data_df['Remedies'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labellization for Train Data.\n",
    "multilabel = MultiLabelBinarizer()\n",
    "d = multilabel.fit_transform(data_df['Remedies'])\n",
    "label_df = pd.DataFrame(d,columns=multilabel.classes_)\n",
    "label_df['Labels'] = label_df.values.tolist()\n",
    "label_df = label_df.drop(list(label_df.columns)[:-1],axis = 1)\n",
    "\n",
    "# Concating Label list to the Main DataFrame\n",
    "data_df = pd.concat([data_df, label_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(list([item for sublist in data_df['Remedies'] for item in sublist]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into Train and Train Dataset\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation part (Synonymical) for Train Data.\n",
    "print('Length before Data Augmentation of Train Data:',len(train_df))\n",
    "for s in range(len(train_df)):\n",
    "    synonyms = []\n",
    "    for k in train_df['PSE'][s].split(', '):\n",
    "        for syn in wordnet.synsets(k):\n",
    "            for i in syn.lemmas():\n",
    "                if k !=i.name():\n",
    "                    synonyms.append([k,i.name()])\n",
    "    for sn in synonyms:\n",
    "        train_df = train_df.append({'Comments':train_df['Comments'].iloc[s],'DateTime':train_df['DateTime'].iloc[s],'Components':train_df['Components'].iloc[s],'PSE':', '.join(list(set(list(train_df['PSE'].iloc[s].replace(str(sn[0]),str(sn[1])).split(', '))))),'PosTag_Remedies':train_df['PosTag_Remedies'].iloc[s],'Remedies':train_df['Remedies'].iloc[s],'Labels':train_df['Labels'].iloc[s]},ignore_index=True)\n",
    "\n",
    "# Normalization, concating of two input columns into one(as CONTEXT) for Train Data.\n",
    "train_df['CONTEXT'] = train_df['Components']+', '+train_df['PSE']\n",
    "temp_clm = train_df.pop(\"CONTEXT\")\n",
    "train_df.insert(6, \"CONTEXT\", temp_clm)\n",
    "\n",
    "# Exploding the Labels Column for Train Dataset.\n",
    "train_df[list(multilabel.classes_)] = pd.DataFrame(train_df.Labels.tolist(),index=train_df.index)\n",
    "train_df = train_df.drop(['Labels'],axis=1)\n",
    "train_df['Remedies'] = train_df['Remedies'].astype(str)\n",
    "# Removing duplicates\n",
    "train_df = train_df.drop_duplicates(ignore_index=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Length after Data Augmentation of Train Data:',len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization, concating of two input columns into one(as CONTEXT) for Test Data.\n",
    "test_df['CONTEXT'] = test_df['Components']+', '+test_df['PSE']\n",
    "temp_clm = test_df.pop(\"CONTEXT\")\n",
    "test_df.insert(6, \"CONTEXT\", temp_clm)\n",
    "\n",
    "# Exploding the Labels Column for Test Dataset.\n",
    "test_df[list(multilabel.classes_)] = pd.DataFrame(test_df.Labels.tolist(),index=test_df.index)\n",
    "test_df = test_df.drop(['Labels'],axis=1)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "test_df['Remedies'] = test_df['Remedies'].astype(str)\n",
    "\n",
    "\n",
    "# Removing duplicates\n",
    "test_df = test_df.drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total of each Outputs in the Train Set.\n",
    "lab_val = []\n",
    "\n",
    "for k in list(train_df.columns)[7:]:\n",
    "    if sum(list(train_df[k])) != 1:\n",
    "        lab_val.append([k,sum(list(train_df[k]))])\n",
    "labl_train_cnt_df = pd.DataFrame(lab_val,columns=['Label Name','Label Count'])\n",
    "labl_train_cnt_df.to_csv('Train_Count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total of each Outputs in the Train Set.\n",
    "count_ls = {}\n",
    "\n",
    "for k in list(test_df.columns)[7:]:\n",
    "    count_ls[k] = sum(list(test_df[k]))\n",
    "count_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving it Datasets in the form of Train and Test for further Model Training.\n",
    "train_df.to_csv('data/Train.csv')\n",
    "test_df.to_csv('data/Test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "252189e587d1e2aeba4a06e91fa71896c7a7f6e22e918b9407c7cde4ef2d5985"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
