{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","Citation:\n","\n","[1] “Bert,” BERT. [Online]. Available: https://huggingface.co/docs/transformers/model_doc/bert.\n","[2] “PyTorch,” torch. [Online]. Available: https://pytorch.org/docs/stable/torch.html.\n","[3] “Pandas,” pandas. [Online]. Available: https://pandas.pydata.org/.\n","[4] “SKLearn,” scikit-learn. [Online]. Available: https://scikit-learn.org/stable/.\n","[5] \"XLNet.\" [Online]. Available: https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/xlnet#overview. \n","[6] V. PRASANNA KUMAR and T. Patro, “Bert model with 0.845 accuracy,” Kaggle, 23-Aug-2020. [Online]. Available: https://www.kaggle.com/code/vpkprasanna/bert-model-with-0-845-accuracy/notebook.\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5143,"status":"ok","timestamp":1658453534934,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"GkTc_uIjrxJN","outputId":"57242ed9-44ce-42d2-8ed2-c4688587570d"},"outputs":[],"source":["# !pip install transformers\n","# !pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3270,"status":"ok","timestamp":1658453538197,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"8egMsXqvriB5"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import shutil\n","import sys \n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1676,"status":"ok","timestamp":1658453539860,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"4PzQdqWYrndg","outputId":"7fdfc3b0-dde0-4209-bc68-cc34b5fafbd3"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658453539861,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"QYOeIIcbriB-","outputId":"6ec3ffd0-fcbd-4ce3-8fa2-cf3daef86014"},"outputs":[],"source":["# Checking the availability og GPU using Pytorch Cuda.\n","\n","if torch.cuda.is_available():       \n","    dev = torch.device(\"cuda\")\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    dev = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658453539861,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"UaIRPbz9riB_"},"outputs":[],"source":["# train_df = pd.read_csv('/content/drive/MyDrive/Univ-Project/Train.csv',index_col=0)\n","train_df = pd.read_csv('data/Train.csv',index_col=0)\n","# test_df = pd.read_csv('/content/drive/MyDrive/Univ-Project/Test.csv',index_col=0)\n","test_df =  pd.read_csv('data/Test.csv',index_col=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658453539862,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"JYc-vXp_riB_"},"outputs":[],"source":["X = train_df.CONTEXT.values\n","y = train_df[list(train_df.columns)[7:]].values\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2020)\n","MAX_LEN = 500"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3454,"status":"ok","timestamp":1658453543310,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"Ovm8fqkxriCA"},"outputs":[],"source":["from transformers import XLNetTokenizer, XLNetModel\n","\n","# Loading the XLNet tokenizer for tokenizing data making it suitable for XLNet Input.\n","\n","TOK = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=False)\n","\n","# Tokenizing texts.\n","def XLNet_Preprocess(data):\n","    inp_ids = []\n","    a_m = []\n","    for sent in data:\n","        encoded_sent = TOK.encode_plus(text = sent,add_special_tokens = True, max_length= MAX_LEN,pad_to_max_length = True,return_attention_mask= True)\n","\n","        # Adding the outputs to lists\n","        inp_ids.append(encoded_sent.get('input_ids'))\n","        a_m.append(encoded_sent.get('attention_mask'))\n","        \n","    # Converting the lists to tensors\n","    inp_ids = torch.tensor(inp_ids)\n","    a_m = torch.tensor(a_m)\n","    \n","    return inp_ids,a_m"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1517,"status":"ok","timestamp":1658453544815,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"yAabwvVRriCB","outputId":"d8438882-3332-43cf-d670-7a71f9154758"},"outputs":[],"source":["# Doing preprocessing for XLnet on the Trainset and Validationset\n","print('Tokenizing data...')\n","train_inp, train_m = XLNet_Preprocess(X_train)\n","val_inp, val_masks = XLNet_Preprocess(X_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1658453544816,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"dWeMFwvbriCB"},"outputs":[],"source":["# Fine-tuning XLnet with a batch size of 16.\n","batch_size = 16\n","\n","# Creating the DataLoader for our Trainset\n","train_data = TensorDataset(train_inp,train_m, torch.tensor(y_train))\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Doing the same for Validationset\n","val_data = TensorDataset(val_inp, val_masks, torch.tensor(y_val))\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1658453544817,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"rEntl9orriCC"},"outputs":[],"source":["class Classifier_fn(nn.Module):\n","    def __init__(self, freeze_bert=False):\n","        super(Classifier_fn,self).__init__()\n","        D_in, H,D_out = 768,30,len(y[0])\n","        self.bert = XLNetModel.from_pretrained('xlnet-base-cased')\n","        \n","        self.classifier = nn.Sequential(\n","                            nn.Linear(D_in, H),\n","                            nn.ReLU(),\n","                            nn.Linear(H, D_out))\n","        self.sigmoid = nn.Sigmoid()\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","    \n","    def forward(self,inp_ids,a_m):\n","        outputs = self.bert(input_ids=inp_ids,\n","                           attention_mask = a_m)\n","        last_hidden_state_cls = outputs[0][:,0,:]\n","        logit = self.classifier(last_hidden_state_cls)        \n","        return logit"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1658453544817,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"tBLRjzDxriCD"},"outputs":[],"source":["BCE_loss = nn.BCEWithLogitsLoss()\n","\n","def train(optimizer,scheduler,model, train_dataloader, val_dataloader, epochs, evaluation,t_l,v_a,v_l,time_elap):\n","    for epoch_i in range(epochs):\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^10} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","\n","        t0_epoch, t0_batch = time.time(), time.time()\n","\n","        # Resetting the tracking variables\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","\n","        model.train()\n","\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            b_inp_ids, b_attn_mask, b_labels = tuple(t.to(dev) for t in batch)\n","\n","            model.zero_grad()\n","\n","            logits = model(b_inp_ids, b_attn_mask)\n","            loss = BCE_loss(logits, b_labels.float())\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","\n","            # Performing Backward pass to calculate gradients\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","            if (step % 50000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","  \n","                time_elapsed = time.time() - t0_batch\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","\n","                # Resetting the batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","\n","        # Calculating the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        print(\"-\"*70)\n","        if evaluation == True:\n","            val_loss, val_accuracy = val_test_eval(model, val_dataloader)\n","            time_elapsed = time.time() - t0_epoch\n","            time_elap.append(time_elapsed)\n","\n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","\n","            # Appending Train Loss, Validation Loss and Validation Accuracy to the General List\n","            t_l.append(avg_train_loss)\n","            v_a.append(val_accuracy)\n","            v_l.append(val_loss)\n","\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","    return model\n","\n","\n","def val_test_eval(model, val_dataloader):\n","    model.eval()\n","    val_accuracy = []\n","    val_loss = []\n","    for batch in val_dataloader:\n","        # Loading the batch to GPU\n","        b_inp_ids, b_attn_mask, b_labels = tuple(t.to(dev) for t in batch)\n","\n","        # Computing logits\n","        with torch.no_grad():\n","            logits = model(b_inp_ids, b_attn_mask)\n","\n","        # Calculating loss\n","        loss = BCE_loss(logits, b_labels.float())\n","        val_loss.append(loss.item())\n","        accuracy = acc_with_thresh(logits.view(-1,len(y[0])),b_labels.view(-1,len(y[0])))\n","        \n","        val_accuracy.append(accuracy)\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy\n","\n","def acc_with_thresh(y_pred, y_true, thresh:float=0.5, sigmoid:bool=True):\n","    if sigmoid: \n","        y_pred = y_pred.sigmoid()\n","    return ((y_pred>thresh)==y_true.byte()).float().mean().item()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1658453544818,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"NPD8msq2riCE"},"outputs":[],"source":["def predict_fn(model, test_dataloader):\n","    model.eval()\n","\n","    all_logits = []\n","\n","    for batch in test_dataloader:\n","        b_inp_ids, b_attn_mask = tuple(t.to(dev) for t in batch)[:2]\n","\n","        # Computing logits\n","        with torch.no_grad():\n","            logits = model(b_inp_ids, b_attn_mask)\n","        all_logits.append(logits)\n","    \n","    # Concatenating logits from each batch\n","    all_logits = torch.cat(all_logits, dim=0)\n","    probs = all_logits.sigmoid().cpu().numpy()\n","    \n","    return probs"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1658453544818,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"etOZVfjEriCE"},"outputs":[],"source":["def model_init(train_dataloader,epochs=4):\n","    # Initiating XLNet Classifier\n","    XLNet_classifier = Classifier_fn(freeze_bert=False)\n","    \n","    XLNet_classifier.to(dev)\n","    \n","    # Defining Optimizer Adam with weight decay\n","    optimizer = AdamW(XLNet_classifier.parameters(),lr=5e-5,eps=1e-8)\n","    \n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","    \n","    # Setting up the LR scheduler\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,num_training_steps=total_steps)\n","    return XLNet_classifier, optimizer, scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8224500,"status":"ok","timestamp":1658461769308,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"uZuG1i9NriCF","outputId":"54f35824-7ded-435a-fb25-1532eae111e6"},"outputs":[],"source":["val_loss = []\n","train_loss = []\n","val_acc= []\n","time_elap = []\n","torch.cuda.empty_cache()\n","# Training the XLNet Classifier on the entire training data\n","XLNet_classifier, optimizer, scheduler = model_init(train_dataloader,epochs=10)\n","model = train(optimizer,scheduler,XLNet_classifier, train_dataloader,val_dataloader,10,True,train_loss,val_acc,val_loss,time_elap)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1658461769310,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"Hszem-5iriCF","outputId":"1604c7d1-162d-4579-e207-acdd65cfbc85"},"outputs":[],"source":["# Preprocessing for XLNet on the Testset\n","print('Tokenizing data...')\n","test_inputs, test_masks = XLNet_Preprocess(test_df.CONTEXT)\n","test_labels = torch.tensor(list(test_df[list(test_df.columns)[7:]].values))\n","test_dataset = TensorDataset(test_inputs, test_masks,test_labels)\n","test_sampler = SequentialSampler(test_dataset)\n","test_dataloader = DataLoader(test_dataset,sampler = test_sampler,batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57315,"status":"ok","timestamp":1658461826619,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"r2a2pqMmriCG"},"outputs":[],"source":["# Predicting probabilities on the test set\n","probs = predict_fn(XLNet_classifier, test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1658461826619,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"8qfE2ncEriCG","outputId":"0735542c-a89d-4bd5-8f0f-2a18f8069b06"},"outputs":[],"source":["y_true = test_df[list(test_df.columns)[7:]].values\n","y_true"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":522},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1658461826621,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"7klgCvKxriCG","outputId":"da8b1ff0-0e6b-41cc-c6a8-18163d302c2b"},"outputs":[],"source":["output_df = pd.DataFrame(probs, columns = list(train_df.columns)[7:]) \n","output_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1658461826622,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"C8WWlBxBsCRe","outputId":"251764c5-7389-4122-d427-adff8c9e04c1"},"outputs":[],"source":["y_pred=[]\n","for sample in  probs:\n","  y_pred.append([1 if i>=0.3 else 0 for i in sample])\n","y_pred = np.array(y_pred)\n","y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56246,"status":"ok","timestamp":1658461882845,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"2xkjk6tqsDDx","outputId":"cb0a6b99-968a-4ebd-c64d-a519fee179a0"},"outputs":[],"source":["# Evaluation\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics\n","\n","Test_loss, Test_accuracy = evaluate(model, test_dataloader)\n","Test_loss = \"{:.0%}\".format(Test_loss)\n","Test_accuracy = \"{:.0%}\".format(Test_accuracy)\n","Prec = \"{:.0%}\".format(metrics.precision_score(y_true, y_pred, average='micro'))\n","Recall = \"{:.0%}\".format( metrics.recall_score(y_true, y_pred, average='micro'))\n","F1_Score = \"{:.0%}\".format(metrics.f1_score(y_true, y_pred, average='micro'))\n","\n","print(Prec)\n","print(Recall)\n","print(F1_Score)\n","print(Test_loss)\n","print(Test_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1658462436505,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"PuQBmeNZsFx1","outputId":"e43d96ab-3b3a-403b-94c8-0ac4fae63c81"},"outputs":[],"source":["fin1 = pd.DataFrame([['Precision',Prec],['Recall',Recall],['F1-Score',F1_Score],['Test-Accuracy',Test_accuracy],['Test-Loss',Test_loss]],columns=[' ','XLNet'])\n","fin1.to_csv('/content/drive/MyDrive/Univ-Project/XLNet_Eval.csv')\n","fin1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":1238,"status":"ok","timestamp":1658461885531,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"OoCJy-jksRLV","outputId":"1ccf2e0d-85ad-40ca-9be7-97fbb9ab3b46"},"outputs":[],"source":["display = pd.DataFrame(columns=['Epochs','Train_Loss','Validation_Accuracy','Validation_Loss','Time-Elapsed'])\n","display['Epochs'] = range(1,11)\n","display['Train_Loss'] = train_loss\n","display['Validation_Accuracy'] = val_acc\n","display['Validation_Loss'] = val_loss\n","display['Time-Elapsed'] = time_elap\n","display.to_csv('/content/drive/MyDrive/Univ-Project/XLNet_Train_Evaluation.csv')\n","display"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":762,"status":"ok","timestamp":1658461885930,"user":{"displayName":"kaivil brahmbhatt","userId":"11781230450915929359"},"user_tz":240},"id":"kAoelu9qsS6h","outputId":"501b3384-1be7-4669-a55c-6d9c0ffcb364"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","final_report = classification_report(y_true, y_pred,target_names=list(train_df.columns)[7:],output_dict=True)\n","report_df = pd.DataFrame(final_report).transpose()\n","report_df = report_df.sort_values(by=['f1-score'], ascending=False)\n","report_df.to_csv('/content/drive/MyDrive/Univ-Project/XLNet_Report_Label.csv')\n","report_df"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"XLNet_std.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"252189e587d1e2aeba4a06e91fa71896c7a7f6e22e918b9407c7cde4ef2d5985"}}},"nbformat":4,"nbformat_minor":0}
