{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Citations :\n",
    "[1] “Beautiful Soup documentation,” Beautiful Soup 4.4.0 documentation. [Online]. Available: https://beautiful-soup-4.readthedocs.io/en/latest/.\n",
    "[2] \"Selenium.\" [Online]. Available: https://www.selenium.dev/. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common import exceptions \n",
    "import datetime\n",
    "from html.parser import HTMLParser\n",
    "from re import sub\n",
    "from sys import stderr\n",
    "from traceback import print_exc\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woLs = []\n",
    "cmnts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for op in opiC.Generic_Name:\n",
    "for op in pd.read_csv('final_opioid_keywords.csv').Generic_Name:\n",
    "    url = \"https://old.reddit.com/r/\"+op+\"/\"\n",
    "    page = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    chk_18 = soup.find_all(\"button\",class_=\"c-btn c-btn-primary\") or 'NONE'\n",
    "    if chk_18 != 'NONE':\n",
    "        driver = webdriver.Chrome(\"driver/chromedriver.exe\")\n",
    "        driver.get(url)\n",
    "        driver.find_elements_by_name('over18')[1].click()\n",
    "        counter = 1\n",
    "        nr11 = driver.find_elements_by_css_selector('a.title.may-blank') or driver.find_elements_by_css_selector('a.title.may-blank.outbound') or [\"NONE\"]\n",
    "        while counter<100:\n",
    "            if nr11[0] != 'NONE':\n",
    "                if counter == 1:\n",
    "                    for nl1 in nr11:\n",
    "                        if nl1 != 'NONE':\n",
    "                            woLs.append([op,nl1.text])\n",
    "                            c_L3 = driver.find_element_by_link_text(nl1.text)\n",
    "                            page = requests.get(c_L3.get_attribute('href'))\n",
    "                            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                            chkCmnts = soup.find_all('div', class_=\"commentarea\")\n",
    "                            cmnts.append([op,chkCmnts]) \n",
    "                    counter = counter+1\n",
    "                else:\n",
    "                    try:\n",
    "                        nL = driver.find_element_by_css_selector('.next-button [href]').get_attribute('href')\n",
    "                        driver.get(nL)\n",
    "                        nr12 = driver.find_elements_by_css_selector('a.title.may-blank') or driver.find_elements_by_css_selector('a.title.may-blank.outbound') or [\"NONE\"]\n",
    "                        for nl2 in nr12:\n",
    "                            if nl2 != 'NONE':\n",
    "                                woLs.append([op,nl2.text])\n",
    "                                c_L3 = driver.find_element_by_link_text(nl2.text)\n",
    "                                page = requests.get(c_L3.get_attribute('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                                soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                                chkCmnts = soup.find_all('div', class_=\"commentarea\")\n",
    "                                cmnts.append([op,chkCmnts]) \n",
    "                        counter = counter+1\n",
    "                    except:\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        driver.close()\n",
    "    else:\n",
    "        driver = webdriver.Chrome(\"driver/chromedriver.exe\")\n",
    "        driver.get(url)\n",
    "        counter = 1\n",
    "        nr11 = driver.find_elements_by_css_selector('a.title.may-blank') or driver.find_elements_by_css_selector('a.title.may-blank.outbound') or [\"NONE\"]\n",
    "        while counter<100:\n",
    "            if nr11[0] != \"NONE\":\n",
    "                if counter == 1:\n",
    "                    for nl1 in nr11:\n",
    "                        if nl1 != 'NONE':\n",
    "                            woLs.append([op,nl1.text])\n",
    "                            c_L3 = driver.find_element_by_link_text(nl1.text)\n",
    "                            print(c_L3.get_attribute('href'))\n",
    "                            page = requests.get(c_L3.get_attribute('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                            chkCmnts = soup.find_all('div', class_=\"commentarea\")\n",
    "                            cmnts.append([op,chkCmnts])\n",
    "                    counter = counter+1\n",
    "                else:\n",
    "                    try:\n",
    "                        nL = driver.find_element_by_css_selector('.next-button [href]').get_attribute('href')\n",
    "                        driver.get(nL)\n",
    "                        nr12 = driver.find_elements_by_css_selector('a.title.may-blank') or driver.find_elements_by_css_selector('a.title.may-blank.outbound') or [\"NONE\"]\n",
    "                        for nl2 in nr12:\n",
    "                            if nl2 != 'NONE':\n",
    "                                woLs.append([op,nl2.text])\n",
    "                                c_L3 = driver.find_element_by_link_text(nl2.text)\n",
    "                                page = requests.get(c_L3.get_attribute('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                                soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                                chkCmnts = soup.find_all('div', class_=\"commentarea\")\n",
    "                                cmnts.append([op,chkCmnts]) \n",
    "                        counter = counter+1\n",
    "                    except:\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinRed = woLs+cmnts\n",
    "FinRedOpi = pd.DataFrame(FinRed,columns=['Components','Comments'])\n",
    "len(FinRedOpi['Comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twisS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for twis in range(len(FinRedOpi)):\n",
    "    print(twis) \n",
    "    htmlParse = BeautifulSoup(FinRedOpi.Comments[twis], 'html.parser')\n",
    "    for jkl in htmlParse.find_all('div',class_='entry unvoted'):\n",
    "        if jkl.find('div',class_='md') != None:\n",
    "            twisS.append([jkl.find('div',class_='md').text, jkl.find('time')['datetime'],FinRedOpi.Components[twis]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCmntsOpi = pd.DataFrame(twisS,columns=['Comments','DateTime','Components'])\n",
    "newCmntsOpi = newCmntsOpi.dropna()\n",
    "newCmntsOpi['Comments'] = [re.sub(r\"\\[.*\\n?\",\"\",str(x),flags=re.MULTILINE) for x in newCmntsOpi['Comments']]\n",
    "newCmntsOpi['Comments'] = newCmntsOpi['Comments'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "newCmntsOpi['Comments'] = newCmntsOpi['Comments'].str.split('\\n\\n')\n",
    "newCmntsOpi['Comments'] = newCmntsOpi['Comments'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
    "newCmntsOpi['Comments'] = newCmntsOpi['Comments'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "newCmntsOpi['Comments'] = newCmntsOpi['Comments'].str.rstrip()\n",
    "newCmntsOpi['Comments'] = newCmntsOpi['Comments'].str.lstrip()\n",
    "newCmntsOpi = newCmntsOpi.dropna()\n",
    "newCmntsOpi = newCmntsOpi.reset_index(drop=True)\n",
    "len(newCmntsOpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCmntsOpi.to_csv('webdata/reddit_effects.csv')\n",
    "newCmntsOpi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "252189e587d1e2aeba4a06e91fa71896c7a7f6e22e918b9407c7cde4ef2d5985"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
