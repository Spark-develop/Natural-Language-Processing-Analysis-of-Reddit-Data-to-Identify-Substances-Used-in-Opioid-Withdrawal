{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common import exceptions \n",
    "import datetime\n",
    "from html.parser import HTMLParser\n",
    "from re import sub\n",
    "from sys import stderr\n",
    "from traceback import print_exc\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woLs = []\n",
    "cmnts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://old.reddit.com/r/OpiatesRecovery/\"\n",
    "page = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "chk_18 = soup.find_all(\"button\",class_=\"c-btn c-btn-primary\") or 'NONE'\n",
    "if chk_18 != 'NONE':\n",
    "    driver = webdriver.Chrome(\"driver/chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    driver.find_elements_by_name('over18')[1].click()\n",
    "    counter = 1\n",
    "    nr11 = driver.find_elements_by_css_selector('div.top-matter') or [\"NONE\"]\n",
    "    nr23 = driver.find_elements_by_css_selector('a.title.may-blank') or driver.find_elements_by_css_selector('a.title.may-blank.outbound') or [\"NONE\"]\n",
    "    while counter<100:\n",
    "        if nr11[0] != \"NONE\":\n",
    "            if counter == 1:\n",
    "                for nl1 in range(len(nr11)):\n",
    "                    if nr11[nl1] != 'NONE':\n",
    "                        woLs.append([nr23[nl1].text,nr11[nl1].find_element_by_css_selector('time').get_attribute(\"datetime\")])\n",
    "                        c_L3 = driver.find_element_by_link_text(nr23[nl1].text)\n",
    "                        print(c_L3.get_attribute('href'))\n",
    "                        page = requests.get(c_L3.get_attribute('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                        chkCmnts = soup.find_all('div', class_=\"commentarea\")\n",
    "                        cmnts.append(chkCmnts)\n",
    "                counter = counter+1\n",
    "            else:\n",
    "                try:\n",
    "                    nL = driver.find_element_by_css_selector('.next-button [href]').get_attribute('href')\n",
    "                    driver.get(nL)\n",
    "                    nr12 = driver.find_elements_by_css_selector('div.top-matter') or [\"NONE\"]\n",
    "                    nr23 = driver.find_elements_by_css_selector('a.title.may-blank') or driver.find_elements_by_css_selector('a.title.may-blank.outbound') or [\"NONE\"]\n",
    "                    for nl2 in range(len(nr12)):\n",
    "                        if nl2 != 'NONE':\n",
    "                            woLs.append([nr23[nl2].text,nr12[nl2].find_element_by_css_selector('time').get_attribute(\"datetime\")])\n",
    "                            c_L3 = driver.find_element_by_link_text(nr23[nl2].text)\n",
    "                            print(c_L3.get_attribute('href'))\n",
    "                            page = requests.get(c_L3.get_attribute('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                            chkCmnts = soup.find_all('div', class_=\"commentarea\")\n",
    "                            cmnts.append(chkCmnts) \n",
    "                    counter = counter+1\n",
    "                except:\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "    driver.close()\n",
    "else:\n",
    "    driver = webdriver.Chrome(\"driver/chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    counter = 1\n",
    "    nr11 = driver.find_elements_by_css_selector('div.top-matter') or [\"NONE\"]\n",
    "    nr23 = driver.find_elements_by_css_selector('a.title.may-blank') or driver.find_elements_by_css_selector('a.title.may-blank.outbound') or [\"NONE\"]\n",
    "    while counter<100:\n",
    "        if nr11[0] != \"NONE\":\n",
    "            if counter == 1:\n",
    "                for nl1 in range(len(nr11)):\n",
    "                    if nr11[nl1] != 'NONE':\n",
    "                        woLs.append([nr23[nl1].text,nr11[nl1].find_element_by_css_selector('time').get_attribute(\"datetime\")])\n",
    "                        c_L3 = driver.find_element_by_link_text(nr23[nl1].text)\n",
    "                        print(c_L3.get_attribute('href'))\n",
    "                        page = requests.get(c_L3.get_attribute('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                        chkCmnts = soup.find_all('div', class_=\"commentarea\")\n",
    "                        cmnts.append(chkCmnts)\n",
    "                counter = counter+1\n",
    "            else:\n",
    "                try:\n",
    "                    nL = driver.find_element_by_css_selector('.next-button [href]').get_attribute('href')\n",
    "                    driver.get(nL)\n",
    "                    nr12 = driver.find_elements_by_css_selector('div.top-matter') or [\"NONE\"]\n",
    "                    nr23 = driver.find_elements_by_css_selector('a.title.may-blank') or driver.find_elements_by_css_selector('a.title.may-blank.outbound') or [\"NONE\"]\n",
    "                    for nl2 in range(len(nr12)):\n",
    "                        if nl2 != 'NONE':\n",
    "                            woLs.append([nr23[nl2].text,nr12[nl2].find_element_by_css_selector('time').get_attribute(\"datetime\")])\n",
    "                            c_L3 = driver.find_element_by_link_text(nr23[nl2].text)\n",
    "                            print(c_L3.get_attribute('href'))\n",
    "                            page = requests.get(c_L3.get_attribute('href'), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "                            chkCmnts = soup.find_all('div', class_=\"commentarea\")\n",
    "                            cmnts.append(chkCmnts) \n",
    "                    counter = counter+1\n",
    "                except:\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwCmnts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kli in cmnts:\n",
    "        for jkl in kli[0].find_all('div',class_='entry unvoted'):\n",
    "                if jkl.find('div',class_='md') != None:\n",
    "                        nwCmnts.append([jkl.find('div',class_='md').text, jkl.find('time')['datetime']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nwCmnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinRedOpi = pd.DataFrame(woLs+nwCmnts,columns=['Comments','DateTime'])\n",
    "len(FinRedOpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCmntsOpi = FinRedOpi.dropna()\n",
    "newCmntsOpi = newCmntsOpi.reset_index(drop=True)\n",
    "len(newCmntsOpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the new Lines from the Comments of r/OpiatesRecovery/\n",
    "newCmntsOpi['Comments'] = newCmntsOpi['Comments'].replace('\\\\n',\" \",regex=True)\n",
    "\n",
    "# Removing the emojis\n",
    "newCmntsOpi = newCmntsOpi.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list3 = []\n",
    "new_list4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ops in pd.read_csv('final_opioid_keywords.csv').Generic_Name:\n",
    "    try:\n",
    "        for j in newCmntsOpi[newCmntsOpi['Comments'].str.contains(ops,na=False)].values.tolist():\n",
    "            if j is not None or j != 'None':\n",
    "                new_list3.append([ops,j])\n",
    "    except:\n",
    "        pass\n",
    "for cv in new_list3:\n",
    "    new_list4.append([cv[0],cv[1][0],cv[1][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCmntsOpi.to_csv('webdata/Opiate-Recovery.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "252189e587d1e2aeba4a06e91fa71896c7a7f6e22e918b9407c7cde4ef2d5985"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
